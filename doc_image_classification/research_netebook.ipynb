{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dbcf482",
   "metadata": {},
   "source": [
    "# Fine-Tuning + Inference API "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61a19020",
   "metadata": {},
   "source": [
    "## 1. Подготовка данных. Для обучения использован поднабор RVL-CDIP (тестовая часть), так как официальный train set недоступен. Разделение выполнено стратифицированно"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f362a43a",
   "metadata": {},
   "source": [
    "### Создание датасета для тренировки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "310fb698",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "096e709b",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['invoice', 'letter', 'email', 'news_article']\n",
    "number_of_files = 2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a80a217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.chdir('data')\n",
    "# os.mkdir('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a516b44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = os.path.join('data', 'train')\n",
    "test_path = os.path.join('data', 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb93a9f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ilgiz/ml-engineering-portfolio/doc_image_classification'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# os.chdir(os.path.join('..', '..'))\n",
    "# os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cd5d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.chdir(dest_path)\n",
    "# for i in range(len(classes)):\n",
    "#     os.mkdir(classes[i])\n",
    "\n",
    "# os.chdir(os.path.join('..', '..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "189d4bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ilgiz/ml-engineering-portfolio/doc_image_classification\n"
     ]
    }
   ],
   "source": [
    "os.chdir(os.path.join('..', '..'))\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "859f8a21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/ilgiz/ml-engineering-portfolio/doc_image_classification'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# os.listdir(os.path.join(src_path, classes[0]))[0]\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b70dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join(train_path, classes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4aefbd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/train/invoice/ti16311152.tif'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = os.listdir(os.path.join(test_path, classes[0]))[0]\n",
    "shutil.copy(os.path.join(test_path, classes[0], file), os.path.join(train_path, classes[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e16c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for c in classes:\n",
    "#     for n in range(number_of_files):\n",
    "#         file = os.listdir(os.path.join(src_path, c))[n]\n",
    "#         shutil.copy(os.path.join(src_path, c, file), os.path.join(dest_path, c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bc5fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: invoice | amount of objects: 2000\n",
      "Class: letter | amount of objects: 2000\n",
      "Class: email | amount of objects: 2000\n",
      "Class: news_article | amount of objects: 2000\n"
     ]
    }
   ],
   "source": [
    "for c in classes:\n",
    "    print(f'Class: {c} | amount of objects: {len(os.listdir(os.path.join(train_path, c)))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8da8ae8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ti16311152.tif'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(os.path.join(test_path, classes[0]))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a981f59b",
   "metadata": {},
   "source": [
    "### Создание `transform` и своего класса `Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdbfb6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from torchvision.transforms import v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e5d18a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = v2.Compose([\n",
    "    v2.ToImage(),    # Конвертируем PIL -> image\n",
    "    v2.ToDtype(torch.float32, scale=True),   # Масштабируем [0, 255] -> [0, 1]\n",
    "    v2.Resize((224, 224), antialias=True), \n",
    "    v2.RandomAffine(degrees=3, translate=(.05, .05), scale=(.95, 1.05)),    # лёгкий поворот + сдвиг + масштаб\n",
    "    v2.GaussianNoise(mean=.0, sigma=.01),\n",
    "    v2.Normalize([.5, .5, .5], [.5, .5, .5])\n",
    "])\n",
    "\n",
    "transform_val = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Resize((224, 224), antialias=True),\n",
    "    v2.Normalize([.5, .5, .5], [.5, .5, .5])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7257448c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class doc_ds(Dataset):\n",
    "    def __init__(self, image_paths, labels, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img = Image.open(self.image_paths[index]).convert('RGB')\n",
    "        if self.transform: img = self.transform(img)\n",
    "        label = torch.tensor(self.labels[index], dtype=torch.long)\n",
    "        \n",
    "        return img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a9210b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_docs = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c5a5631",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t1 = pd.DataFrame(columns=['doc_path', 'label'])\n",
    "df_t2 = pd.DataFrame(columns=df_t1.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3c246ca0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['invoice', 'letter', 'email', 'news_article']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "68437fe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/train/invoice'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_rpath = os.path.relpath(os.path.join(train_path, classes[0]))\n",
    "class_rpath"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90535eff",
   "metadata": {},
   "source": [
    "### Загрузка файлов и разделение на `train`, `validation`, `test`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1900a138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/train/invoice/ti16311152.tif</td>\n",
       "      <td>invoice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/train/invoice/2084022630.tif</td>\n",
       "      <td>invoice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/train/invoice/2063235294.tif</td>\n",
       "      <td>invoice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/train/invoice/83545557.tif</td>\n",
       "      <td>invoice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/train/invoice/2029370755.tif</td>\n",
       "      <td>invoice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7995</th>\n",
       "      <td>data/train/news_article/tob14401.20.tif</td>\n",
       "      <td>news_article</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7996</th>\n",
       "      <td>data/train/news_article/1003289799.tif</td>\n",
       "      <td>news_article</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7997</th>\n",
       "      <td>data/train/news_article/2048367429.tif</td>\n",
       "      <td>news_article</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998</th>\n",
       "      <td>data/train/news_article/1002402701a.tif</td>\n",
       "      <td>news_article</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7999</th>\n",
       "      <td>data/train/news_article/ton00509.94.tif</td>\n",
       "      <td>news_article</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         path        labels\n",
       "0           data/train/invoice/ti16311152.tif       invoice\n",
       "1           data/train/invoice/2084022630.tif       invoice\n",
       "2           data/train/invoice/2063235294.tif       invoice\n",
       "3             data/train/invoice/83545557.tif       invoice\n",
       "4           data/train/invoice/2029370755.tif       invoice\n",
       "...                                       ...           ...\n",
       "7995  data/train/news_article/tob14401.20.tif  news_article\n",
       "7996   data/train/news_article/1003289799.tif  news_article\n",
       "7997   data/train/news_article/2048367429.tif  news_article\n",
       "7998  data/train/news_article/1002402701a.tif  news_article\n",
       "7999  data/train/news_article/ton00509.94.tif  news_article\n",
       "\n",
       "[8000 rows x 2 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tvt_files_df = pd.DataFrame(columns=['path', 'labels'])\n",
    "for c in classes:\n",
    "    class_rpath = os.path.relpath(os.path.join(train_path, c))\n",
    "    temp_df = pd.DataFrame(columns=['path', 'labels'])\n",
    "    temp_df['path'] = class_rpath + '/' + pd.Series(os.listdir(os.path.join(train_path, c)))\n",
    "    temp_df['labels'] = c\n",
    "    \n",
    "    tvt_files_df = pd.concat((tvt_files_df, temp_df), axis=0, ignore_index=True)\n",
    "\n",
    "tvt_files_df\n",
    "\n",
    "# tvt_files_df # train, validate, test - data\n",
    "# tvt_files_df\n",
    "\n",
    "\n",
    "# invoice_files = pd.Series(os.listdir(os.path.join(train_path, classes[0])))\n",
    "# letter_files = pd.Series(os.listdir(os.path.join(train_path, classes[1])))\n",
    "\n",
    "# pd.concat((invoice_files, letter_files), ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "321384ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "11ab1072",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>labels</th>\n",
       "      <th>label_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data/train/invoice/ti16311152.tif</td>\n",
       "      <td>invoice</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data/train/invoice/2084022630.tif</td>\n",
       "      <td>invoice</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data/train/invoice/2063235294.tif</td>\n",
       "      <td>invoice</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data/train/invoice/83545557.tif</td>\n",
       "      <td>invoice</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data/train/invoice/2029370755.tif</td>\n",
       "      <td>invoice</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7995</th>\n",
       "      <td>data/train/news_article/tob14401.20.tif</td>\n",
       "      <td>news_article</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7996</th>\n",
       "      <td>data/train/news_article/1003289799.tif</td>\n",
       "      <td>news_article</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7997</th>\n",
       "      <td>data/train/news_article/2048367429.tif</td>\n",
       "      <td>news_article</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7998</th>\n",
       "      <td>data/train/news_article/1002402701a.tif</td>\n",
       "      <td>news_article</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7999</th>\n",
       "      <td>data/train/news_article/ton00509.94.tif</td>\n",
       "      <td>news_article</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         path        labels  label_id\n",
       "0           data/train/invoice/ti16311152.tif       invoice         1\n",
       "1           data/train/invoice/2084022630.tif       invoice         1\n",
       "2           data/train/invoice/2063235294.tif       invoice         1\n",
       "3             data/train/invoice/83545557.tif       invoice         1\n",
       "4           data/train/invoice/2029370755.tif       invoice         1\n",
       "...                                       ...           ...       ...\n",
       "7995  data/train/news_article/tob14401.20.tif  news_article         3\n",
       "7996   data/train/news_article/1003289799.tif  news_article         3\n",
       "7997   data/train/news_article/2048367429.tif  news_article         3\n",
       "7998  data/train/news_article/1002402701a.tif  news_article         3\n",
       "7999  data/train/news_article/ton00509.94.tif  news_article         3\n",
       "\n",
       "[8000 rows x 3 columns]"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tvt_files_df['label_id'] = le.fit_transform(tvt_files_df['labels'])\n",
    "tvt_files_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "216706eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 0, 3])"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tvt_files_df['label_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73693c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "d5572637",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_doc_ds = doc_ds(tvt_files_df.path.to_list(), tvt_files_df.label_id.to_list(), transform=transform_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "049dd2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds, test_ds = random_split(full_doc_ds, [.7, .15, .15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "a78e49d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "val_dl = DataLoader(val_ds, batch_size=batch_size, shuffle=True)\n",
    "test_dl = DataLoader(test_ds, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd13cc9e",
   "metadata": {},
   "source": [
    "## fine-tuning модели"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686d410d",
   "metadata": {},
   "source": [
    "### Загрузка модели, заморозка слоев, добавление поледнего линейного слоя"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d90e46ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import timm\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "62e7c809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "url: https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/efficientnet_b0_ra-3dd342df.pth\n",
      "hf_hub_id: timm/efficientnet_b0.ra_in1k\n",
      "architecture: efficientnet_b0\n",
      "tag: ra_in1k\n",
      "custom_load: False\n",
      "input_size: (3, 224, 224)\n",
      "fixed_input_size: False\n",
      "interpolation: bicubic\n",
      "crop_pct: 0.875\n",
      "crop_mode: center\n",
      "mean: (0.485, 0.456, 0.406)\n",
      "std: (0.229, 0.224, 0.225)\n",
      "num_classes: 1000\n",
      "pool_size: (7, 7)\n",
      "first_conv: conv_stem\n",
      "classifier: classifier\n",
      "license: apache-2.0\n"
     ]
    }
   ],
   "source": [
    "model = timm.create_model('efficientnet_b0', pretrained=True)\n",
    "\n",
    "\n",
    "for param in model.parameters():    # Замораживаем слои\n",
    "    param.requires_grad = False\n",
    "\n",
    "model.classifier = nn.Linear(model.classifier.in_features, 4)     # Добавляем новый полносвязный слой с активными градиентами\n",
    "\n",
    "\n",
    "# Обображаем параметры модели\n",
    "def show_model_info(model):\n",
    "    for k, v in model.default_cfg.items():\n",
    "        print(f'{k}: {v}')\n",
    "\n",
    "\n",
    "show_model_info(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "cc2cdccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False, False, False, True, True]\n"
     ]
    }
   ],
   "source": [
    "print([p.requires_grad for p in model.parameters()][-5:])  # последние 5 тензоров — должны быть True (голова)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91831e1d",
   "metadata": {},
   "source": [
    "### Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "cabf70fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 6\n",
    "learning_rate = 1e-3    # 1e-3 - 3e-4\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.classifier.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "77b1a996",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "fc14452f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "8eb61fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, loader, optimizer, criterion):\n",
    "    total_loss, correct_preds = 0, 0\n",
    "    for imgs, targets in loader:\n",
    "        imgs = imgs.to(device); targets = targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        preds = model(imgs)\n",
    "        loss = criterion(preds, targets)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * imgs.size(0)\n",
    "        correct_preds += (preds.argmax(dim=1) == targets).sum().item()\n",
    "        \n",
    "    return total_loss / len(loader.dataset), correct_preds / len(loader.dataset)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "e70f7307",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, loader, criterion):\n",
    "    total_loss, correct_preds = 0, 0\n",
    "    torch.inference_mode()\n",
    "    for imgs, targets in loader:\n",
    "        imgs = imgs.to(device); targets = targets.to(device)\n",
    "        \n",
    "        preds = model(imgs)\n",
    "        loss = criterion(preds, targets)\n",
    "        \n",
    "        total_loss += loss.item() * imgs.size(0)\n",
    "        correct_preds += (preds.argmax(dim=1) == targets).sum().item()  \n",
    "        \n",
    "    \n",
    "    return total_loss / len(loader.dataset), correct_preds / len(loader.dataset)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186650b2",
   "metadata": {},
   "source": [
    "### Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "8b35b947",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm    # Полоска загрузки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcb5fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/6 | Train accuracy: 0.0000%, Train loss: 0.7872 | Validation accuracy: 0.0000%, Validation loss: 0.6482\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[158]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[32m      5\u001b[39m     train_loss, train_acc = train_model(model, train_dl, optimizer, criterion)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     val_loss, val_acc = \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | Train accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%, Train loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m | Validation accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m%, Validation loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[157]\u001b[39m\u001b[32m, line 4\u001b[39m, in \u001b[36mevaluate_model\u001b[39m\u001b[34m(model, loader, criterion)\u001b[39m\n\u001b[32m      2\u001b[39m total_loss, correct_preds = \u001b[32m0\u001b[39m, \u001b[32m0\u001b[39m\n\u001b[32m      3\u001b[39m torch.inference_mode()\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m;\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargets\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/ml-engineering-portfolio-PXL5yKuT-py3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/ml-engineering-portfolio-PXL5yKuT-py3.12/lib/python3.12/site-packages/torch/utils/data/dataloader.py:789\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    787\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    788\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m789\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    790\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    791\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/ml-engineering-portfolio-PXL5yKuT-py3.12/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.auto_collation:\n\u001b[32m     49\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m.dataset, \u001b[33m\"\u001b[39m\u001b[33m__getitems__\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__:\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     52\u001b[39m         data = [\u001b[38;5;28mself\u001b[39m.dataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/ml-engineering-portfolio-PXL5yKuT-py3.12/lib/python3.12/site-packages/torch/utils/data/dataset.py:416\u001b[39m, in \u001b[36mSubset.__getitems__\u001b[39m\u001b[34m(self, indices)\u001b[39m\n\u001b[32m    414\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dataset.__getitems__([\u001b[38;5;28mself\u001b[39m.indices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m    415\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m416\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[101]\u001b[39m\u001b[32m, line 12\u001b[39m, in \u001b[36mdoc_ds.__getitem__\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[32m     11\u001b[39m     img = Image.open(\u001b[38;5;28mself\u001b[39m.image_paths[index]).convert(\u001b[33m'\u001b[39m\u001b[33mRGB\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transform: img = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m     label = torch.tensor(\u001b[38;5;28mself\u001b[39m.labels[index])\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m img, label\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/ml-engineering-portfolio-PXL5yKuT-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/ml-engineering-portfolio-PXL5yKuT-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/ml-engineering-portfolio-PXL5yKuT-py3.12/lib/python3.12/site-packages/torchvision/transforms/v2/_container.py:51\u001b[39m, in \u001b[36mCompose.forward\u001b[39m\u001b[34m(self, *inputs)\u001b[39m\n\u001b[32m     49\u001b[39m needs_unpacking = \u001b[38;5;28mlen\u001b[39m(inputs) > \u001b[32m1\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transforms:\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m     outputs = \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     52\u001b[39m     inputs = outputs \u001b[38;5;28;01mif\u001b[39;00m needs_unpacking \u001b[38;5;28;01melse\u001b[39;00m (outputs,)\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/ml-engineering-portfolio-PXL5yKuT-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/ml-engineering-portfolio-PXL5yKuT-py3.12/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/ml-engineering-portfolio-PXL5yKuT-py3.12/lib/python3.12/site-packages/torchvision/transforms/v2/_transform.py:69\u001b[39m, in \u001b[36mTransform.forward\u001b[39m\u001b[34m(self, *inputs)\u001b[39m\n\u001b[32m     63\u001b[39m needs_transform_list = \u001b[38;5;28mself\u001b[39m._needs_transform_list(flat_inputs)\n\u001b[32m     64\u001b[39m params = \u001b[38;5;28mself\u001b[39m.make_params(\n\u001b[32m     65\u001b[39m     [inpt \u001b[38;5;28;01mfor\u001b[39;00m (inpt, needs_transform) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(flat_inputs, needs_transform_list) \u001b[38;5;28;01mif\u001b[39;00m needs_transform]\n\u001b[32m     66\u001b[39m )\n\u001b[32m     68\u001b[39m flat_outputs = [\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43minpt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m needs_transform \u001b[38;5;28;01melse\u001b[39;00m inpt\n\u001b[32m     70\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m (inpt, needs_transform) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(flat_inputs, needs_transform_list)\n\u001b[32m     71\u001b[39m ]\n\u001b[32m     73\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tree_unflatten(flat_outputs, spec)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/ml-engineering-portfolio-PXL5yKuT-py3.12/lib/python3.12/site-packages/torchvision/transforms/v2/_geometry.py:160\u001b[39m, in \u001b[36mResize.transform\u001b[39m\u001b[34m(self, inpt, params)\u001b[39m\n\u001b[32m    159\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, inpt: Any, params: Dict[\u001b[38;5;28mstr\u001b[39m, Any]) -> Any:\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_kernel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    161\u001b[39m \u001b[43m        \u001b[49m\u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[43m        \u001b[49m\u001b[43minpt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    163\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    164\u001b[39m \u001b[43m        \u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    166\u001b[39m \u001b[43m        \u001b[49m\u001b[43mantialias\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mantialias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    167\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/ml-engineering-portfolio-PXL5yKuT-py3.12/lib/python3.12/site-packages/torchvision/transforms/v2/_transform.py:49\u001b[39m, in \u001b[36mTransform._call_kernel\u001b[39m\u001b[34m(self, functional, inpt, *args, **kwargs)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_call_kernel\u001b[39m(\u001b[38;5;28mself\u001b[39m, functional: Callable, inpt: Any, *args: Any, **kwargs: Any) -> Any:\n\u001b[32m     48\u001b[39m     kernel = _get_kernel(functional, \u001b[38;5;28mtype\u001b[39m(inpt), allow_passthrough=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mkernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minpt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/ml-engineering-portfolio-PXL5yKuT-py3.12/lib/python3.12/site-packages/torchvision/transforms/v2/functional/_utils.py:31\u001b[39m, in \u001b[36m_kernel_tv_tensor_wrapper.<locals>.wrapper\u001b[39m\u001b[34m(inpt, *args, **kwargs)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(kernel)\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(inpt, *args, **kwargs):\n\u001b[32m     22\u001b[39m     \u001b[38;5;66;03m# If you're wondering whether we could / should get rid of this wrapper,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     29\u001b[39m     \u001b[38;5;66;03m# lost after the first operation due to our own __torch_function__\u001b[39;00m\n\u001b[32m     30\u001b[39m     \u001b[38;5;66;03m# logic.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m     output = \u001b[43mkernel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minpt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mas_subclass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTensor\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tv_tensors.wrap(output, like=inpt)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/ml-engineering-portfolio-PXL5yKuT-py3.12/lib/python3.12/site-packages/torchvision/transforms/v2/functional/_geometry.py:260\u001b[39m, in \u001b[36mresize_image\u001b[39m\u001b[34m(image, size, interpolation, max_size, antialias)\u001b[39m\n\u001b[32m    257\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m need_cast:\n\u001b[32m    258\u001b[39m     image = image.to(dtype=torch.float32)\n\u001b[32m--> \u001b[39m\u001b[32m260\u001b[39m image = \u001b[43minterpolate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    262\u001b[39m \u001b[43m    \u001b[49m\u001b[43msize\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnew_height\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_width\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    263\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[43minterpolation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[43m    \u001b[49m\u001b[43malign_corners\u001b[49m\u001b[43m=\u001b[49m\u001b[43malign_corners\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[43m    \u001b[49m\u001b[43mantialias\u001b[49m\u001b[43m=\u001b[49m\u001b[43mantialias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m need_cast:\n\u001b[32m    269\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m interpolation == InterpolationMode.BICUBIC \u001b[38;5;129;01mand\u001b[39;00m dtype == torch.uint8:\n\u001b[32m    270\u001b[39m         \u001b[38;5;66;03m# This path is hit on non-AVX archs, or on GPU.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/ml-engineering-portfolio-PXL5yKuT-py3.12/lib/python3.12/site-packages/torch/nn/functional.py:4678\u001b[39m, in \u001b[36minterpolate\u001b[39m\u001b[34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor, antialias)\u001b[39m\n\u001b[32m   4676\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m align_corners \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4677\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m antialias:\n\u001b[32m-> \u001b[39m\u001b[32m4678\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_upsample_bilinear2d_aa\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   4679\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign_corners\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_factors\u001b[49m\n\u001b[32m   4680\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4681\u001b[39m \u001b[38;5;66;03m# Two levels are necessary to prevent TorchScript from touching\u001b[39;00m\n\u001b[32m   4682\u001b[39m \u001b[38;5;66;03m# are_deterministic_algorithms_enabled.\u001b[39;00m\n\u001b[32m   4683\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch.jit.is_scripting():\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "total_train_acc, total_val_acc = [], []\n",
    "total_train_loss, total_val_loss = [], []\n",
    "\n",
    "for e in range(epochs):\n",
    "    train_loss, train_acc = train_model(model, train_dl, optimizer, criterion)\n",
    "    val_loss, val_acc = evaluate_model(model, val_dl, criterion)\n",
    "    \n",
    "    total_train_acc.append(train_acc); total_train_loss.append(train_loss)\n",
    "    total_val_acc.append(val_acc); total_val_loss.append(val_acc)\n",
    "    \n",
    "    print(f\"Epoch: {e+1}/{epochs} | Train accuracy: {train_acc:.4f}%, Train loss: {train_loss:.4f} | Validation accuracy: {val_acc:.4f}%, Validation loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec78a4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ac8593",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decc337d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec5f5ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-engineering-portfolio-PXL5yKuT-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
